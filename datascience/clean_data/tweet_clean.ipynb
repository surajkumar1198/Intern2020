{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import  matplotlib.pyplot as plt\n",
    "import os\n",
    "import emoji\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "from langdetect import detect\n",
    "from bs4 import BeautifulSoup\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emoticons\n",
    "def load_dict_smileys():\n",
    "    \n",
    "    return {\n",
    "        \":‑)\":\"smiley\",\n",
    "        \":-]\":\"smiley\",\n",
    "        \":-3\":\"smiley\",\n",
    "        \":->\":\"smiley\",\n",
    "        \"8-)\":\"smiley\",\n",
    "        \":-}\":\"smiley\",\n",
    "        \":)\":\"smiley\",\n",
    "        \":]\":\"smiley\",\n",
    "        \":3\":\"smiley\",\n",
    "        \":>\":\"smiley\",\n",
    "        \"8)\":\"smiley\",\n",
    "        \":}\":\"smiley\",\n",
    "        \":o)\":\"smiley\",\n",
    "        \":c)\":\"smiley\",\n",
    "        \":^)\":\"smiley\",\n",
    "        \"=]\":\"smiley\",\n",
    "        \"=)\":\"smiley\",\n",
    "        \":-))\":\"smiley\",\n",
    "        \":‑D\":\"smiley\",\n",
    "        \"8‑D\":\"smiley\",\n",
    "        \"x‑D\":\"smiley\",\n",
    "        \"X‑D\":\"smiley\",\n",
    "        \":D\":\"smiley\",\n",
    "        \"8D\":\"smiley\",\n",
    "        \"xD\":\"smiley\",\n",
    "        \"XD\":\"smiley\",\n",
    "        \":‑(\":\"sad\",\n",
    "        \":‑c\":\"sad\",\n",
    "        \":‑<\":\"sad\",\n",
    "        \":‑[\":\"sad\",\n",
    "        \":(\":\"sad\",\n",
    "        \":c\":\"sad\",\n",
    "        \":<\":\"sad\",\n",
    "        \":[\":\"sad\",\n",
    "        \":-||\":\"sad\",\n",
    "        \">:[\":\"sad\",\n",
    "        \":{\":\"sad\",\n",
    "        \":@\":\"sad\",\n",
    "        \">:(\":\"sad\",\n",
    "        \":'‑(\":\"sad\",\n",
    "        \":'(\":\"sad\",\n",
    "        \":‑P\":\"playful\",\n",
    "        \"X‑P\":\"playful\",\n",
    "        \"x‑p\":\"playful\",\n",
    "        \":‑p\":\"playful\",\n",
    "        \":‑Þ\":\"playful\",\n",
    "        \":‑þ\":\"playful\",\n",
    "        \":‑b\":\"playful\",\n",
    "        \":P\":\"playful\",\n",
    "        \"XP\":\"playful\",\n",
    "        \"xp\":\"playful\",\n",
    "        \":p\":\"playful\",\n",
    "        \":Þ\":\"playful\",\n",
    "        \":þ\":\"playful\",\n",
    "        \":b\":\"playful\",\n",
    "        \"<3\":\"love\"\n",
    "        }\n",
    "\n",
    "# self defined contractions\n",
    "def load_dict_contractions():\n",
    "    \n",
    "    return {\n",
    "        \"ain't\":\"is not\",\n",
    "        \"amn't\":\"am not\",\n",
    "        \"aren't\":\"are not\",\n",
    "        \"can't\":\"cannot\",\n",
    "        \"'cause\":\"because\",\n",
    "        \"couldn't\":\"could not\",\n",
    "        \"couldn't've\":\"could not have\",\n",
    "        \"could've\":\"could have\",\n",
    "        \"daren't\":\"dare not\",\n",
    "        \"daresn't\":\"dare not\",\n",
    "        \"dasn't\":\"dare not\",\n",
    "        \"didn't\":\"did not\",\n",
    "        \"doesn't\":\"does not\",\n",
    "        \"don't\":\"do not\",\n",
    "        \"e'er\":\"ever\",\n",
    "        \"em\":\"them\",\n",
    "        \"everyone's\":\"everyone is\",\n",
    "        \"finna\":\"fixing to\",\n",
    "        \"gimme\":\"give me\",\n",
    "        \"gonna\":\"going to\",\n",
    "        \"gon't\":\"go not\",\n",
    "        \"gotta\":\"got to\",\n",
    "        \"hadn't\":\"had not\",\n",
    "        \"hasn't\":\"has not\",\n",
    "        \"haven't\":\"have not\",\n",
    "        \"he'd\":\"he would\",\n",
    "        \"he'll\":\"he will\",\n",
    "        \"he's\":\"he is\",\n",
    "        \"he've\":\"he have\",\n",
    "        \"how'd\":\"how would\",\n",
    "        \"how'll\":\"how will\",\n",
    "        \"how're\":\"how are\",\n",
    "        \"how's\":\"how is\",\n",
    "        \"I'd\":\"I would\",\n",
    "        \"I'll\":\"I will\",\n",
    "        \"I'm\":\"I am\",\n",
    "        \"I'm'a\":\"I am about to\",\n",
    "        \"I'm'o\":\"I am going to\",\n",
    "        \"isn't\":\"is not\",\n",
    "        \"it'd\":\"it would\",\n",
    "        \"it'll\":\"it will\",\n",
    "        \"it's\":\"it is\",\n",
    "        \"I've\":\"I have\",\n",
    "        \"kinda\":\"kind of\",\n",
    "        \"let's\":\"let us\",\n",
    "        \"mayn't\":\"may not\",\n",
    "        \"may've\":\"may have\",\n",
    "        \"mightn't\":\"might not\",\n",
    "        \"might've\":\"might have\",\n",
    "        \"mustn't\":\"must not\",\n",
    "        \"mustn't've\":\"must not have\",\n",
    "        \"must've\":\"must have\",\n",
    "        \"needn't\":\"need not\",\n",
    "        \"ne'er\":\"never\",\n",
    "        \"o'\":\"of\",\n",
    "        \"o'er\":\"over\",\n",
    "        \"ol'\":\"old\",\n",
    "        \"oughtn't\":\"ought not\",\n",
    "        \"shalln't\":\"shall not\",\n",
    "        \"shan't\":\"shall not\",\n",
    "        \"she'd\":\"she would\",\n",
    "        \"she'll\":\"she will\",\n",
    "        \"she's\":\"she is\",\n",
    "        \"shouldn't\":\"should not\",\n",
    "        \"shouldn't've\":\"should not have\",\n",
    "        \"should've\":\"should have\",\n",
    "        \"somebody's\":\"somebody is\",\n",
    "        \"someone's\":\"someone is\",\n",
    "        \"something's\":\"something is\",\n",
    "        \"that'd\":\"that would\",\n",
    "        \"that'll\":\"that will\",\n",
    "        \"that're\":\"that are\",\n",
    "        \"that's\":\"that is\",\n",
    "        \"there'd\":\"there would\",\n",
    "        \"there'll\":\"there will\",\n",
    "        \"there're\":\"there are\",\n",
    "        \"there's\":\"there is\",\n",
    "        \"these're\":\"these are\",\n",
    "        \"they'd\":\"they would\",\n",
    "        \"they'll\":\"they will\",\n",
    "        \"they're\":\"they are\",\n",
    "        \"they've\":\"they have\",\n",
    "        \"this's\":\"this is\",\n",
    "        \"those're\":\"those are\",\n",
    "        \"'tis\":\"it is\",\n",
    "        \"'twas\":\"it was\",\n",
    "        \"wanna\":\"want to\",\n",
    "        \"wasn't\":\"was not\",\n",
    "        \"we'd\":\"we would\",\n",
    "        \"we'd've\":\"we would have\",\n",
    "        \"we'll\":\"we will\",\n",
    "        \"we're\":\"we are\",\n",
    "        \"weren't\":\"were not\",\n",
    "        \"we've\":\"we have\",\n",
    "        \"what'd\":\"what did\",\n",
    "        \"what'll\":\"what will\",\n",
    "        \"what're\":\"what are\",\n",
    "        \"what's\":\"what is\",\n",
    "        \"what've\":\"what have\",\n",
    "        \"when's\":\"when is\",\n",
    "        \"where'd\":\"where did\",\n",
    "        \"where're\":\"where are\",\n",
    "        \"where's\":\"where is\",\n",
    "        \"where've\":\"where have\",\n",
    "        \"which's\":\"which is\",\n",
    "        \"who'd\":\"who would\",\n",
    "        \"who'd've\":\"who would have\",\n",
    "        \"who'll\":\"who will\",\n",
    "        \"who're\":\"who are\",\n",
    "        \"who's\":\"who is\",\n",
    "        \"who've\":\"who have\",\n",
    "        \"why'd\":\"why did\",\n",
    "        \"why're\":\"why are\",\n",
    "        \"why's\":\"why is\",\n",
    "        \"won't\":\"will not\",\n",
    "        \"wouldn't\":\"would not\",\n",
    "        \"would've\":\"would have\",\n",
    "        \"y'all\":\"you all\",\n",
    "        \"you'd\":\"you would\",\n",
    "        \"you'll\":\"you will\",\n",
    "        \"you're\":\"you are\",\n",
    "        \"you've\":\"you have\",\n",
    "        \"Whatcha\":\"What are you\",\n",
    "        \"luv\":\"love\",\n",
    "        \"sux\":\"sucks\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweet_cleaning_for_sentiment_analysis(tweet):    \n",
    "    tweet=str(tweet)\n",
    "    #Escaping HTML characters\n",
    "    tweet = BeautifulSoup(tweet).get_text()\n",
    "   \n",
    "    #Special case not handled previously.\n",
    "#     tweet = tweet.replace('\\x92',\"'\")\n",
    "    \n",
    "    #Removal of hastags/account\n",
    "    tweet = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(#[A-Za-z0-9]+)\", \" \", tweet).split())\n",
    "    \n",
    "    #Removal of address\n",
    "    tweet = ' '.join(re.sub(\"(\\w+:\\/\\/\\S+)\", \" \", tweet).split())\n",
    "    \n",
    "    #Removal of Punctuation\n",
    "    tweet = ' '.join(re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=]\", \" \", tweet).split())\n",
    "    \n",
    "    #Lower case\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    #CONTRACTIONS source: https://en.wikipedia.org/wiki/Contraction_%28grammar%29\n",
    "    CONTRACTIONS = load_dict_contractions()\n",
    "    tweet = tweet.replace(\"’\",\"'\")\n",
    "    words = tweet.split()\n",
    "    reformed = [CONTRACTIONS[word] if word in CONTRACTIONS else word for word in words]\n",
    "    tweet = \" \".join(reformed)\n",
    "    \n",
    "    # Standardizing words\n",
    "    tweet = ''.join(''.join(s)[:2] for _, s in itertools.groupby(tweet))\n",
    "    \n",
    "    #Deal with emoticons source: https://en.wikipedia.org/wiki/List_of_emoticons\n",
    "    SMILEY = load_dict_smileys()  \n",
    "    words = tweet.split()\n",
    "    reformed = [SMILEY[word] if word in SMILEY else word for word in words]\n",
    "    tweet = \" \".join(reformed)\n",
    "    \n",
    "    #Deal with emojis\n",
    "    tweet = emoji.demojize(tweet)\n",
    "#     print(tweet)\n",
    "    tweet = tweet.replace(\":\",\" \")\n",
    "    tweet = ' '.join(tweet.split())\n",
    "\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectLang(text):\n",
    "    try:\n",
    "        if detect(text) == 'en':\n",
    "            return text\n",
    "        else :\n",
    "            return None\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read raw datas from the raw data file\n",
    "path = r'D:\\Covid19Data\\TrendingTopicData\\\\'\n",
    "files = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in files:\n",
    "    data = pd.read_csv(str(path) + file)\n",
    "    data['clean_text']=data['text'].apply(tweet_cleaning_for_sentiment_analysis)\n",
    "    data['clean_text']=data['clean_text'].apply(lambda x:detectLang(x))\n",
    "    data = data[data.clean_text.notnull()]\n",
    "    outputPath=r'D:\\Covid19Data\\processedData\\\\'\n",
    "    data.to_csv(str(outputPath)+file+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>favorites</th>\n",
       "      <th>retweets</th>\n",
       "      <th>mentions</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>fahad_dean</td>\n",
       "      <td>POLICE CATCHING PEOPLE IN THE LOCK DOWN PRANK ...</td>\n",
       "      <td>2020-04-23 23:57:23+00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#crazypranktv #fahaddean #Pranks #pranksinpaki...</td>\n",
       "      <td>police catching people in the lock down prank ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>pinankjoshi</td>\n",
       "      <td>Corona virus first key reason to start world w...</td>\n",
       "      <td>2020-04-23 23:56:51+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>corona virus first key reason to start world war</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>bisu_mangal_09</td>\n",
       "      <td>Life goes on, with or without you😉 #instagood ...</td>\n",
       "      <td>2020-04-23 23:53:13+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#instagood #photooftheday #selfie #stayhome #s...</td>\n",
       "      <td>life goes on with or without you winking_face ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>_mahanagi</td>\n",
       "      <td>Uni? The whole world got into a lockdown. Matl...</td>\n",
       "      <td>2020-04-23 23:52:50+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uni the whole world got into a lockdown matlab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>pinankjoshi</td>\n",
       "      <td>Approx on average temperature in India near to...</td>\n",
       "      <td>2020-04-23 23:50:35+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>approx on average temperature in india near to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4995</td>\n",
       "      <td>RajperRaza12345</td>\n",
       "      <td>Request to Government of Sindh Plz issue offer...</td>\n",
       "      <td>2020-04-23 05:32:05+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>request to government of sindh plz issue offer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4996</td>\n",
       "      <td>bukitima</td>\n",
       "      <td>This is not going to help the post pandemic gl...</td>\n",
       "      <td>2020-04-23 05:31:59+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this is not going to help the post pandemic gl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4997</td>\n",
       "      <td>Pjthara</td>\n",
       "      <td>Real heroes! #COVID19 #Frontliners</td>\n",
       "      <td>2020-04-23 05:31:28+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#COVID19 #Frontliners</td>\n",
       "      <td>real heroes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4998</td>\n",
       "      <td>paepaesays</td>\n",
       "      <td>This is so touching. Go #SGUnited. #Singapore ...</td>\n",
       "      <td>2020-04-23 05:31:15+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#SGUnited #Singapore #COVID19</td>\n",
       "      <td>this is so touching go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4999</td>\n",
       "      <td>sumnyanS</td>\n",
       "      <td>Expansion is Life #Lockdownextention. As a par...</td>\n",
       "      <td>2020-04-23 05:30:44+00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmbujAtreya @arunachaltimes_ @incredibleindia...</td>\n",
       "      <td>#Lockdownextention</td>\n",
       "      <td>expansion is life as a part of nature we need ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4874 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 user                                               text  \\\n",
       "0          fahad_dean  POLICE CATCHING PEOPLE IN THE LOCK DOWN PRANK ...   \n",
       "1         pinankjoshi  Corona virus first key reason to start world w...   \n",
       "2      bisu_mangal_09  Life goes on, with or without you😉 #instagood ...   \n",
       "3           _mahanagi  Uni? The whole world got into a lockdown. Matl...   \n",
       "4         pinankjoshi  Approx on average temperature in India near to...   \n",
       "...               ...                                                ...   \n",
       "4995  RajperRaza12345  Request to Government of Sindh Plz issue offer...   \n",
       "4996         bukitima  This is not going to help the post pandemic gl...   \n",
       "4997          Pjthara                 Real heroes! #COVID19 #Frontliners   \n",
       "4998       paepaesays  This is so touching. Go #SGUnited. #Singapore ...   \n",
       "4999         sumnyanS  Expansion is Life #Lockdownextention. As a par...   \n",
       "\n",
       "                           date  favorites  retweets  \\\n",
       "0     2020-04-23 23:57:23+00:00          2        11   \n",
       "1     2020-04-23 23:56:51+00:00          0         0   \n",
       "2     2020-04-23 23:53:13+00:00          0         0   \n",
       "3     2020-04-23 23:52:50+00:00          0         0   \n",
       "4     2020-04-23 23:50:35+00:00          0         0   \n",
       "...                         ...        ...       ...   \n",
       "4995  2020-04-23 05:32:05+00:00          1         3   \n",
       "4996  2020-04-23 05:31:59+00:00          0         1   \n",
       "4997  2020-04-23 05:31:28+00:00          0         0   \n",
       "4998  2020-04-23 05:31:15+00:00          0         0   \n",
       "4999  2020-04-23 05:30:44+00:00          0         0   \n",
       "\n",
       "                                               mentions  \\\n",
       "0                                                   NaN   \n",
       "1                                                   NaN   \n",
       "2                                                   NaN   \n",
       "3                                                   NaN   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "4995                                                NaN   \n",
       "4996                                                NaN   \n",
       "4997                                                NaN   \n",
       "4998                                                NaN   \n",
       "4999  @AmbujAtreya @arunachaltimes_ @incredibleindia...   \n",
       "\n",
       "                                               hashtags  \\\n",
       "0     #crazypranktv #fahaddean #Pranks #pranksinpaki...   \n",
       "1                                                   NaN   \n",
       "2     #instagood #photooftheday #selfie #stayhome #s...   \n",
       "3                                                   NaN   \n",
       "4                                                   NaN   \n",
       "...                                                 ...   \n",
       "4995                                                NaN   \n",
       "4996                                                NaN   \n",
       "4997                              #COVID19 #Frontliners   \n",
       "4998                      #SGUnited #Singapore #COVID19   \n",
       "4999                                 #Lockdownextention   \n",
       "\n",
       "                                             clean_text  \n",
       "0     police catching people in the lock down prank ...  \n",
       "1      corona virus first key reason to start world war  \n",
       "2     life goes on with or without you winking_face ...  \n",
       "3     uni the whole world got into a lockdown matlab...  \n",
       "4     approx on average temperature in india near to...  \n",
       "...                                                 ...  \n",
       "4995  request to government of sindh plz issue offer...  \n",
       "4996  this is not going to help the post pandemic gl...  \n",
       "4997                                        real heroes  \n",
       "4998                             this is so touching go  \n",
       "4999  expansion is life as a part of nature we need ...  \n",
       "\n",
       "[4874 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
